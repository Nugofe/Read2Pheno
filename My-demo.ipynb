{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/Read2Pheno-YfFcwwiN/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sequence_attention import SeqAttModel, preprocess_data, preprocess_data_pickle\n",
    "from sequence_attention import DataGenerator, DataGeneratorUnlabeled, DataGeneratorPickle, DataGeneratorUnlabeledPickle\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "There are three different ways to prepare the data: \n",
    "\n",
    "2. convert each fna file to a pickle file with a python dictionary data structure (key is the unique sequence identifier and value is the actual sequence). Then the data generator can load a pickle file and look up for a read and construct a batch of data.\n",
    "3. The fastest way is, of course, training and testing your model without using the data generator. Instead, permitted by the computer memory, users can load all the training data in memory and directly fit the model. In this way, there will be no additional I/O process. \n",
    "\n",
    "When designing this tool, we don't make any assumption on the computer memory at the user's disposal, therefore, this demo focuses on the second preprocessing method since it covers the most use cases. But if the users do have enough memory, they are more than welcome to fit the model without the generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-Jun-22 15:34:32 - Processing raw data: 0.0% completed.\n",
      "02-Jun-22 15:34:32 - Processing raw data: 10.0% completed.\n",
      "02-Jun-22 15:34:32 - Processing raw data: 20.0% completed.\n",
      "02-Jun-22 15:34:33 - Processing raw data: 30.0% completed.\n",
      "02-Jun-22 15:34:33 - Processing raw data: 40.0% completed.\n",
      "02-Jun-22 15:34:33 - Processing raw data: 50.0% completed.\n",
      "02-Jun-22 15:34:34 - Processing raw data: 60.0% completed.\n",
      "02-Jun-22 15:34:34 - Processing raw data: 70.0% completed.\n",
      "02-Jun-22 15:34:35 - Processing raw data: 80.0% completed.\n",
      "02-Jun-22 15:34:35 - Processing raw data: 90.0% completed.\n",
      "02-Jun-22 15:34:36 - Processing raw data: 100.0% completed.\n",
      "02-Jun-22 15:34:37 - Processing raw data: 110.0% completed.\n"
     ]
    }
   ],
   "source": [
    "# cargar a configuraciÃ³n\n",
    "opt = Config()\n",
    "\n",
    "# pre-procesar dos datos\n",
    "preprocess_data_pickle(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metadata and initialize the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar os pickle cos datos pre-procesados no paso anterior\n",
    "label_dict = pickle.load(open('{}/label_dict.pkl'.format(opt.out_dir), 'rb')) \n",
    "sample_to_label, read_meta_data = pickle.load(open('{}/meta_data.pkl'.format(opt.out_dir), 'rb'))\n",
    "#partition = pickle.load(open('{}/train_test_split.pkl'.format(opt.out_dir), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear o modelo\n",
    "seq_att_model = SeqAttModel(opt)\n",
    "seq_att_model.model.summary() # resumo do modelo (capas de cada tipo, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate the model\n",
    "###### Prepare the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding das secuencias\n",
    "generator = DataGeneratorPickle(partition, sample_to_label, label_dict, \n",
    "                                   dim=(opt.SEQLEN,opt.BASENUM), batch_size=opt.batch_size, shuffle=opt.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adestrar batch a batch se os datos non caben en memoria\n",
    "\n",
    "# X = sequences one-hot encoded, y = sequences' labels\n",
    "X, y = training_generator.__getitem__(0) # Generate one batch of data\n",
    "\n",
    "#X.shape  # 1359 batch size (training sequences in the batch), 100 sequence len, 4 bases (unique characters of your data) # sequence len: trim all your reads to the same length or pad zeros to the end so that all the reads are in the same length.\n",
    "          # PRUEBA  ->  12 batch size, 100 seq len, 4 bases\n",
    "\n",
    "#y.shape  # 1359 labels for the training data in the batch\n",
    "          # PRUEBA  ->  12 \n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "#X.shape  # 1024 batch size (training sequences in the batch), 100 sequence len, 4 bases (unique characters of your data) # sequence len: trim all your reads to the same length or pad zeros to the end so that all the reads are in the same length.\n",
    "          # PRUEBA  ->  6 batch size, 100 seq len, 4 bases\n",
    "\n",
    "#y.shape  # 1024 labels for the training data in the batch\n",
    "          # PRUEBA  ->  6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-May-22 15:49:32 - Training started:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 3643/78927 [>.............................] - ETA: 14:36:16 - loss: 0.5809 - acc: 0.6846"
     ]
    }
   ],
   "source": [
    "# NO\n",
    "# adestrar o modelo batch a batch e avaliar a acurracy do train set\n",
    "seq_att_model.train_generator(training_generator, n_workers=opt.n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO\n",
    "# avaliar a acurracy do test set\n",
    "seq_att_model.evaluate_generator(testing_generator, n_workers=opt.n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axustar os datos ao modelo\n",
    "seq_att_model.model.fit(X, y, batch_size=opt.batch_size, epochs=opt.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training without the generator (if you have enough memmory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation and sequence visualization\n",
    "This step is exploratory and completely depends on you. To get started, please review the following data requirements:\n",
    "\n",
    "1. Prepare the X_visual (N by SEQ_LEN by NUMBASE) in *numpy array*, see also ***Note: training without the generator (1)***\n",
    "2. y_visual (phenotypic labels in integers) in *numpy array*, use `label_dict` as the label to integer map.\n",
    "3. a list of taxonomic labels of those sequences (e.g., genus level labels as python strings). \n",
    "\n",
    "Once you have the data ready, run the following commands to plot embedding and attention weights visualization figures.\n",
    "\n",
    "###### Note: we provide a toy example of how it works and what results it produces in the appendix section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sequence attention weigths and sequence embedding from the model for input sequences, X.\n",
    "prediction, attention_weights, sequence_embedding = seq_att_model.extract_weigths(X_visual)\n",
    "\n",
    "from sequence_attention import SeqVisualUnit\n",
    "idx_to_label = {label_dict[label]: label for label in label_dict}\n",
    "seq_visual_unit = SeqVisualUnit(X_visual, y_visual, idx_to_label, taxa_label_list, \n",
    "                                prediction, attention_weights, sequence_embedding, 'Figures')\n",
    "\n",
    "seq_visual_unit.plot_embedding()\n",
    "seq_visual_unit.plot_attention('CD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, we also need , *label_dict* (phenotypic labels to integer dictionary saved in `opt.out_dir/label_dict.pkl` by the previous steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# PRUEBAS\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
